{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asigachev\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\asigachev\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rifmator_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e757eb2e13c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrifmator_engine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRifmator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rifmator_engine'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import LSTM, Dropout, GRU, Embedding, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import random\n",
    "import sys\n",
    "from difflib import SequenceMatcher\n",
    "import operator\n",
    "import gensim\n",
    "from nltk import tokenize\n",
    "from rifmator_engine import Rifmator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsGenModel:\n",
    "    def __init__(self):\n",
    "        self.text_corp = None\n",
    "        self.tokens = None\n",
    "        self.vocab_size = None # Размер словаря - в итоге должен определиться из материала\n",
    "        self.seq_length = None # Длина последовательности слов, на которой обучаемся\n",
    "        self.model = None # Основная модель, по которой генерируется текст\n",
    "        self.generated_flow = None\n",
    "        self.length = None\n",
    "    \n",
    "    def set_length(self, value):\n",
    "        self.length = value\n",
    "    \n",
    "    def load_data(self, data_file_name):\n",
    "        # Функция загружает тексты песен из Pandas Dataframe\n",
    "        print('Загрузка данных')\n",
    "        df = pd.read_csv(data_file_name, encoding='cp1251')\n",
    "        songs_lyrics = list(df['0'])\n",
    "        print('Найдено песен:', len(songs_lyrics))\n",
    "        \n",
    "        self.text_corp = ' '.join(songs_lyrics)    # Создаём общий корпус текста - объединение всех песен \n",
    "        \n",
    "        # Удаляем все спецсимволы и разделяем склеившиеся слова\n",
    "        remove_punct = re.compile('[^а-яА-Я\\s\\dё]') \n",
    "        self.text_corp = re.sub(remove_punct, '', self.text_corp) \n",
    "        garbled_words_regex = re.compile('(?P<A>[А-я][а-я]+)(?P<B>[А-Я][а-я]*)')\n",
    "        self.text_corp = re.sub(garbled_words_regex, '\\g<A> \\g<B>', self.text_corp)\n",
    "        \n",
    "        # Токенизация - разделение текста на слова\n",
    "        self.tokens = self.text_corp.lower().split()\n",
    "        print('Получено токенов из корпуса текста:', len(self.tokens))\n",
    "        \n",
    "        # Получение последовательностей из N токенов. По первым токенам, потом будем предсказывать последний\n",
    "        self.sequences = list()\n",
    "        for i in range(self.length, len(self.tokens)):\n",
    "            # select sequence of tokens\n",
    "            seq = self.tokens[i-self.length:i]\n",
    "            # convert into a line\n",
    "            line = ' '.join(seq)\n",
    "            # store\n",
    "            self.sequences.append(line)\n",
    "        print(self.sequences[:10])    \n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.sequences)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        \n",
    "        print('Размер словаря:', self.vocab_size)\n",
    "\n",
    "        sequences_enc = self.tokenizer.texts_to_sequences(self.sequences)\n",
    "        sequences_enc = np.array(sequences_enc)        \n",
    "        self.X, self.y = sequences_enc[:,:-1], sequences_enc[:,-1]\n",
    "        self.seq_length = self.X.shape[1]\n",
    "        \n",
    "        # Обучение word2vec-модели для выбора рифм\n",
    "        \n",
    "        self.w2v_model = gensim.models.Word2Vec([self.tokens], size=100, window=5, min_count=1, workers=4)\n",
    "       \n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.vocab_size, 50, input_length=self.seq_length))\n",
    "        model.add(GRU(100, return_sequences=True))\n",
    "        model.add(GRU(100))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dense(self.vocab_size, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print('Модель создана:')\n",
    "        print(model.summary())\n",
    "        \n",
    "    def train_model(self, b_size, ep_num):\n",
    "        self.model.fit(self.X, self.y, batch_size=b_size, epochs=ep_num) \n",
    "    \n",
    "    def save_model(self, file_name):\n",
    "        self.model.save(file_name)\n",
    "    \n",
    "    def save_weights(self, file_name):\n",
    "        self.model.save_weights(file_name)\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        self.model = load_model(file_name)\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def load_weights(self, file_name):\n",
    "        self.model.load_weights(file_name)\n",
    "        \n",
    "    def get_seed_text(self):\n",
    "        self.seed_text = self.sequences[random.randint(0,len(self.sequences))]\n",
    "\n",
    "    def sample(self, preds, diversity):\n",
    "        # Функция принимает список индексов и соответствующих им вероятностей\n",
    "        # Нормализует вероятности так, чтобы их сумма была равной 1\n",
    "        # Возвращает индекс с соответствующей ему вероятностью\n",
    "        indexes = preds.argsort()[0][-diversity:]\n",
    "        preds.shape\n",
    "        probs = [float(preds[:,x]) for x in indexes]\n",
    "        print('probabilities:', ['%.2f' % elem for elem in probs])\n",
    "        normalized_probs = [x/sum(probs) for x in probs]\n",
    "        print('normalized_probabilities', ['%.2f' % elem for elem in normalized_probs])\n",
    "        return np.random.choice(indexes, p=normalized_probs)        \n",
    "\n",
    "    def generate_seq(self, n_words):\n",
    "        result = list()\n",
    "        in_text = self.seed_text\n",
    "        # Создаём поток из n_words слов\n",
    "        for _ in range(n_words):\n",
    "            # Кодируем сидирующую последовательность в числа, чтобы подать на вход модели\n",
    "            encoded = self.tokenizer.texts_to_sequences([in_text])[0]\n",
    "            \n",
    "            # Приведение последовательности к необходимой длине\n",
    "            encoded = pad_sequences([encoded], maxlen=self.seq_length, truncating='pre')\n",
    "            \n",
    "            # Предсказываем вероятности для вариантов следующего слова\n",
    "            pred = self.model.predict_proba(encoded, verbose=0)\n",
    "            \n",
    "            yhat = np.argmax(pred)\n",
    "            \n",
    "            # Восстанавливаем слово по целочисленному значению\n",
    "            out_word = ''\n",
    "            for word, index in self.tokenizer.word_index.items():\n",
    "                if index == yhat:\n",
    "                    out_word = word\n",
    "                    break\n",
    "                    \n",
    "            # Приделываем слово к потоку\n",
    "            in_text += ' ' + out_word\n",
    "            result.append(out_word)\n",
    "        print('Результат сохранён в атрибут generated_flow')\n",
    "        self.generated_flow = ' '.join(result)    \n",
    "        \n",
    "    def generate_seq_with_randomness(self, n_words, diversity):\n",
    "        result = list()\n",
    "        in_text = self.seed_text\n",
    "        # Создаём поток из n_words слов\n",
    "        for _ in range(n_words):\n",
    "            # Кодируем сидирующую последовательность в числа, чтобы подать на вход модели\n",
    "            encoded = self.tokenizer.texts_to_sequences([in_text])[0]\n",
    "            \n",
    "            # Приведение последовательности к необходимой длине\n",
    "            encoded = pad_sequences([encoded], maxlen=self.seq_length, truncating='pre')\n",
    "            # Предсказываем вероятности для вариантов следующего слова\n",
    "            pred = self.model.predict_proba(encoded, verbose=0)\n",
    "            \n",
    "            yhat = self.sample(pred, diversity)\n",
    "\n",
    "            # Восстанавливаем слово по целочисленному значению\n",
    "            out_word = ''\n",
    "            for word, index in self.tokenizer.word_index.items():\n",
    "                if index == yhat:\n",
    "                    out_word = word\n",
    "                    break\n",
    "            # append to input\n",
    "            in_text += ' ' + out_word\n",
    "            result.append(out_word)\n",
    "        print('Результат сохранён в атрибут generated_flow')\n",
    "        self.generated_flow = ' '.join(result)      \n",
    "\n",
    "    def get_verses(self, text):\n",
    "        # Функция получает пару рифмованных строк\n",
    "        max_verse_len = 6\n",
    "        min_verse_len = 1\n",
    "        rhyme_threshold = 0.935        \n",
    "        flow_list = text.split()\n",
    "        long_words= [x for x in set(self.tokens) if len(x)>2]\n",
    "        \n",
    "        for i in range(max_verse_len,-1,-1):\n",
    "            for j in range(i+1, i+1+max_verse_len):\n",
    "                #print(flow_list[i], flow_list[j], self.rhyme_score(flow_list[i],flow_list[j], False))\n",
    "                if self.rhyme_score(flow_list[i],flow_list[j], False) > rhyme_threshold:\n",
    "                    verse_a = ' '.join(flow_list[:i+1])\n",
    "                    verse_b = ' '.join(flow_list[i+1:j+1])\n",
    "                    return verse_a, verse_b, verse_b\n",
    "        else:       \n",
    "            \n",
    "            \n",
    "            for i in range(max_verse_len,-1,-1):\n",
    "                word_to_put = None\n",
    "                if len(flow_list[i]) < 3:\n",
    "                    continue\n",
    "                else:\n",
    "                    word_to_rhyme = flow_list[i]\n",
    "                # Список индексов слов в случайном порядке. Будем пытаться найти рифму к словам из этого списка \n",
    "                clist = random.sample([x for x in range(min_verse_len,max_verse_len+1)], max_verse_len-min_verse_len+1)\n",
    "                \n",
    "                \n",
    "        \n",
    "                for c in clist:\n",
    "                    word_to_change = flow_list[i+c] # Получаем слово, которое будем менять\n",
    "                    #print('Пробуем искать рифмы к слову ', word_to_rhyme)\n",
    "                    rhymes_found = self.find_best_rhymes(word_to_rhyme,  long_words, rhyme_threshold, word_to_change) #Ищем лучшие рифмы к слову\n",
    "                    #print('Найдено рифм', len(rhymes_found))\n",
    "                    if len(rhymes_found) > 0: #Если рифма нашлась\n",
    "                        word_to_put = rhymes_found[random.randint(1,len(rhymes_found))-1] #Выбираем слово, которое вставляем\n",
    "                        break\n",
    "                if word_to_put == None:\n",
    "                    continue\n",
    "                #print('Меняем', word_to_change, 'на', word_to_put)\n",
    "                verse_a = text.split(word_to_rhyme)[0]+word_to_rhyme\n",
    "                #print('verse_a', verse_a)\n",
    "                verse_b_orig = text.replace(verse_a+' ', '').split(word_to_change)[0] + word_to_change\n",
    "                #print('verse_b_orig', verse_b_orig)\n",
    "                verse_b_aslist = verse_b_orig.split()\n",
    "                verse_b_aslist[-1] = word_to_put\n",
    "                verse_b = ' '.join(verse_b_aslist)\n",
    "                return verse_a, verse_b, verse_b_orig\n",
    "\n",
    "    def create_verse(self, max_line_len):\n",
    "        # Функция формирует стих\n",
    "        flow = self.generated_flow\n",
    "        verses = list()\n",
    "        self.verses_final = list()\n",
    "        print(len(flow.split()))\n",
    "        while len(flow.split()) > max_line_len*2+1:\n",
    "            new_verses = self.get_verses(flow)\n",
    "            verses.extend(new_verses)\n",
    "            self.verses_final.extend(new_verses[:-1])\n",
    "            flow = flow.split(verses[-1]+' ')[-1]\n",
    "            print(len(flow))\n",
    "        \n",
    "    def similar(self, a, b):\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "    \n",
    "    def rhyme_score(self, a, b, verbose):\n",
    "        score = 0\n",
    "        \n",
    "        # Слова должны быть длиннее 3 символов, для всего, что короче возвращаем 0\n",
    "        if len(a) < 3 or len(b) < 3:\n",
    "            return 0\n",
    "        \n",
    "        # Для одинаковых слов возвращать 0\n",
    "        \n",
    "        if a == b:\n",
    "            return 0\n",
    "        \n",
    "        maxlen = min(len(a),len(b), 4)+1\n",
    "        for i in range(1,maxlen):\n",
    "            score += self.similar(a[-i:], b[-i:])\n",
    "        score = score / (maxlen - 1)   \n",
    "        return score\n",
    "    \n",
    "    def find_best_rhymes(self, word_to_check, words_list, threshold, word_to_change=''):\n",
    "        if len(word_to_check) < 3:\n",
    "            return []\n",
    "        words_set = set(words_list)\n",
    "        potential_rhymes = list()\n",
    "        similarity = 0\n",
    "        for word in words_set:\n",
    "            if self.rhyme_score(word_to_check, word, False) > threshold and word not in word_to_check and word_to_check not in word:# здесь поменять на леммы\n",
    "                potential_rhymes.append(word)\n",
    "                similarity = self.rhyme_score(word_to_check, word, False)\n",
    "\n",
    "        # Если указано слово, которое меняем фильтруем рифмы по близости к слову, которое меняем\n",
    "        if word_to_change !='':\n",
    "            potential_rhymes = self.find_best_rhymes(word_to_check, words_set, threshold)\n",
    "            rhyme_proximity_dict = {}\n",
    "            for i in potential_rhymes:\n",
    "                rhyme_proximity_dict.update({i : self.w2v_model.wv.similarity(word_to_change, i.lower())})\n",
    "            chosen_rhymes = sorted(rhyme_proximity_dict.items(),\n",
    "                                   key=operator.itemgetter(1), reverse=True)[:1+min(len(rhyme_proximity_dict)//3, \n",
    "                                                                                    len(rhyme_proximity_dict))]\n",
    "            return [x[0] for x in chosen_rhymes]\n",
    "        # Если не указано, просто возвращаем список найденных рифм\n",
    "        else:\n",
    "            return potential_rhymes\n",
    "\n",
    "\n",
    "    def print_verses(self):\n",
    "        for verse in self.verses_final:\n",
    "            print(verse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение на хип-хоп-текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel_hiphop = LyricsGenModel() #Создание объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel_hiphop.set_length(4) #Установка длины последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных\n",
      "Найдено песен: 956\n",
      "Получено токенов из корпуса текста: 311707\n",
      "['это мне поручили узнать', 'мне поручили узнать куда', 'поручили узнать куда время', 'узнать куда время течет', 'куда время течет и', 'время течет и когда', 'течет и когда в', 'и когда в истории', 'когда в истории начнется', 'в истории начнется новый']\n",
      "Размер словаря: 51777\n"
     ]
    }
   ],
   "source": [
    "mymodel_hiphop.load_data('lyrics-extended.csv') #Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 3, 50)             2588850   \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 3, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 51777)             5229477   \n",
      "=================================================================\n",
      "Total params: 7,969,227\n",
      "Trainable params: 7,969,227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mymodel_hiphop.load_model('loss15extendedlyr') #Загрузка предобученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel_hiphop.load_weights('loss15extendedlyr-w') #Загрузка весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel_hiphop.get_seed_text() #Получение стартовой последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities: ['0.05', '0.09', '0.18']\n",
      "normalized_probabilities ['0.15', '0.29', '0.57']\n",
      "probabilities: ['0.08', '0.11', '0.57']\n",
      "normalized_probabilities ['0.10', '0.14', '0.75']\n",
      "probabilities: ['0.03', '0.05', '0.90']\n",
      "normalized_probabilities ['0.03', '0.05', '0.92']\n",
      "probabilities: ['0.10', '0.11', '0.16']\n",
      "normalized_probabilities ['0.27', '0.30', '0.43']\n",
      "probabilities: ['0.06', '0.07', '0.85']\n",
      "normalized_probabilities ['0.06', '0.07', '0.87']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.03', '0.10', '0.83']\n",
      "normalized_probabilities ['0.03', '0.10', '0.87']\n",
      "probabilities: ['0.03', '0.04', '0.92']\n",
      "normalized_probabilities ['0.03', '0.04', '0.93']\n",
      "probabilities: ['0.05', '0.08', '0.77']\n",
      "normalized_probabilities ['0.05', '0.09', '0.86']\n",
      "probabilities: ['0.01', '0.08', '0.88']\n",
      "normalized_probabilities ['0.01', '0.08', '0.90']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.12', '0.17', '0.59']\n",
      "normalized_probabilities ['0.14', '0.19', '0.67']\n",
      "probabilities: ['0.04', '0.06', '0.13']\n",
      "normalized_probabilities ['0.19', '0.25', '0.56']\n",
      "probabilities: ['0.08', '0.21', '0.30']\n",
      "normalized_probabilities ['0.14', '0.35', '0.51']\n",
      "probabilities: ['0.05', '0.42', '0.48']\n",
      "normalized_probabilities ['0.05', '0.44', '0.51']\n",
      "probabilities: ['0.03', '0.05', '0.84']\n",
      "normalized_probabilities ['0.04', '0.06', '0.91']\n",
      "probabilities: ['0.10', '0.17', '0.43']\n",
      "normalized_probabilities ['0.14', '0.24', '0.62']\n",
      "probabilities: ['0.04', '0.10', '0.21']\n",
      "normalized_probabilities ['0.12', '0.29', '0.60']\n",
      "probabilities: ['0.11', '0.15', '0.59']\n",
      "normalized_probabilities ['0.13', '0.18', '0.69']\n",
      "probabilities: ['0.03', '0.08', '0.84']\n",
      "normalized_probabilities ['0.03', '0.08', '0.88']\n",
      "probabilities: ['0.09', '0.12', '0.69']\n",
      "normalized_probabilities ['0.10', '0.13', '0.77']\n",
      "probabilities: ['0.13', '0.21', '0.32']\n",
      "normalized_probabilities ['0.19', '0.32', '0.49']\n",
      "probabilities: ['0.12', '0.19', '0.30']\n",
      "normalized_probabilities ['0.20', '0.31', '0.49']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.02', '0.96']\n",
      "normalized_probabilities ['0.00', '0.03', '0.97']\n",
      "probabilities: ['0.08', '0.08', '0.21']\n",
      "normalized_probabilities ['0.21', '0.22', '0.57']\n",
      "probabilities: ['0.10', '0.21', '0.41']\n",
      "normalized_probabilities ['0.14', '0.29', '0.57']\n",
      "probabilities: ['0.04', '0.12', '0.32']\n",
      "normalized_probabilities ['0.08', '0.25', '0.67']\n",
      "probabilities: ['0.07', '0.12', '0.31']\n",
      "normalized_probabilities ['0.14', '0.24', '0.62']\n",
      "probabilities: ['0.04', '0.06', '0.07']\n",
      "normalized_probabilities ['0.26', '0.34', '0.40']\n",
      "probabilities: ['0.06', '0.06', '0.09']\n",
      "normalized_probabilities ['0.29', '0.30', '0.41']\n",
      "probabilities: ['0.04', '0.10', '0.42']\n",
      "normalized_probabilities ['0.07', '0.19', '0.74']\n",
      "probabilities: ['0.05', '0.28', '0.63']\n",
      "normalized_probabilities ['0.05', '0.29', '0.66']\n",
      "probabilities: ['0.01', '0.01', '0.96']\n",
      "normalized_probabilities ['0.01', '0.01', '0.98']\n",
      "probabilities: ['0.07', '0.24', '0.36']\n",
      "normalized_probabilities ['0.11', '0.36', '0.53']\n",
      "probabilities: ['0.09', '0.16', '0.41']\n",
      "normalized_probabilities ['0.13', '0.24', '0.63']\n",
      "probabilities: ['0.09', '0.21', '0.43']\n",
      "normalized_probabilities ['0.12', '0.28', '0.60']\n",
      "probabilities: ['0.09', '0.12', '0.12']\n",
      "normalized_probabilities ['0.27', '0.36', '0.36']\n",
      "probabilities: ['0.05', '0.06', '0.06']\n",
      "normalized_probabilities ['0.30', '0.34', '0.36']\n",
      "probabilities: ['0.00', '0.01', '0.99']\n",
      "normalized_probabilities ['0.00', '0.01', '0.99']\n",
      "probabilities: ['0.01', '0.06', '0.90']\n",
      "normalized_probabilities ['0.01', '0.06', '0.93']\n",
      "probabilities: ['0.09', '0.11', '0.14']\n",
      "normalized_probabilities ['0.26', '0.31', '0.43']\n",
      "probabilities: ['0.06', '0.08', '0.15']\n",
      "normalized_probabilities ['0.20', '0.28', '0.52']\n",
      "probabilities: ['0.05', '0.16', '0.36']\n",
      "normalized_probabilities ['0.09', '0.27', '0.63']\n",
      "probabilities: ['0.03', '0.40', '0.49']\n",
      "normalized_probabilities ['0.04', '0.44', '0.53']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.11', '0.27', '0.51']\n",
      "normalized_probabilities ['0.12', '0.30', '0.57']\n",
      "probabilities: ['0.00', '0.03', '0.95']\n",
      "normalized_probabilities ['0.00', '0.03', '0.96']\n",
      "probabilities: ['0.00', '0.01', '0.99']\n",
      "normalized_probabilities ['0.00', '0.01', '0.99']\n",
      "probabilities: ['0.00', '0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '0.00', '0.99']\n",
      "probabilities: ['0.06', '0.31', '0.55']\n",
      "normalized_probabilities ['0.07', '0.34', '0.59']\n",
      "probabilities: ['0.13', '0.27', '0.51']\n",
      "normalized_probabilities ['0.14', '0.30', '0.56']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.01', '0.03', '0.96']\n",
      "normalized_probabilities ['0.01', '0.03', '0.96']\n",
      "probabilities: ['0.29', '0.30', '0.37']\n",
      "normalized_probabilities ['0.30', '0.31', '0.39']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.01', '0.98']\n",
      "normalized_probabilities ['0.00', '0.01', '0.98']\n",
      "probabilities: ['0.22', '0.25', '0.35']\n",
      "normalized_probabilities ['0.26', '0.31', '0.43']\n",
      "probabilities: ['0.10', '0.11', '0.24']\n",
      "normalized_probabilities ['0.23', '0.25', '0.52']\n",
      "probabilities: ['0.08', '0.10', '0.79']\n",
      "normalized_probabilities ['0.09', '0.10', '0.81']\n",
      "probabilities: ['0.04', '0.05', '0.88']\n",
      "normalized_probabilities ['0.04', '0.05', '0.91']\n",
      "probabilities: ['0.12', '0.13', '0.14']\n",
      "normalized_probabilities ['0.30', '0.34', '0.36']\n",
      "probabilities: ['0.00', '0.46', '0.54']\n",
      "normalized_probabilities ['0.00', '0.46', '0.54']\n",
      "probabilities: ['0.07', '0.10', '0.74']\n",
      "normalized_probabilities ['0.07', '0.11', '0.81']\n",
      "probabilities: ['0.06', '0.13', '0.14']\n",
      "normalized_probabilities ['0.18', '0.40', '0.42']\n",
      "probabilities: ['0.02', '0.31', '0.61']\n",
      "normalized_probabilities ['0.02', '0.33', '0.65']\n",
      "probabilities: ['0.12', '0.16', '0.16']\n",
      "normalized_probabilities ['0.27', '0.37', '0.37']\n",
      "probabilities: ['0.04', '0.06', '0.73']\n",
      "normalized_probabilities ['0.04', '0.07', '0.89']\n",
      "probabilities: ['0.04', '0.05', '0.11']\n",
      "normalized_probabilities ['0.19', '0.24', '0.57']\n",
      "probabilities: ['0.06', '0.08', '0.41']\n",
      "normalized_probabilities ['0.12', '0.14', '0.74']\n",
      "probabilities: ['0.05', '0.09', '0.72']\n",
      "normalized_probabilities ['0.06', '0.10', '0.84']\n",
      "probabilities: ['0.01', '0.01', '0.97']\n",
      "normalized_probabilities ['0.01', '0.01', '0.98']\n",
      "probabilities: ['0.11', '0.18', '0.21']\n",
      "normalized_probabilities ['0.22', '0.36', '0.42']\n",
      "probabilities: ['0.06', '0.14', '0.27']\n",
      "normalized_probabilities ['0.12', '0.29', '0.58']\n",
      "probabilities: ['0.09', '0.09', '0.16']\n",
      "normalized_probabilities ['0.26', '0.28', '0.46']\n",
      "probabilities: ['0.01', '0.15', '0.82']\n",
      "normalized_probabilities ['0.01', '0.16', '0.83']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.02', '0.06', '0.78']\n",
      "normalized_probabilities ['0.02', '0.07', '0.91']\n",
      "probabilities: ['0.08', '0.23', '0.24']\n",
      "normalized_probabilities ['0.14', '0.43', '0.43']\n",
      "probabilities: ['0.11', '0.12', '0.17']\n",
      "normalized_probabilities ['0.28', '0.30', '0.42']\n",
      "probabilities: ['0.02', '0.04', '0.88']\n",
      "normalized_probabilities ['0.02', '0.04', '0.93']\n",
      "probabilities: ['0.08', '0.09', '0.72']\n",
      "normalized_probabilities ['0.09', '0.10', '0.81']\n",
      "probabilities: ['0.00', '0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '0.00', '0.99']\n",
      "probabilities: ['0.14', '0.16', '0.44']\n",
      "normalized_probabilities ['0.19', '0.22', '0.59']\n",
      "probabilities: ['0.07', '0.09', '0.29']\n",
      "normalized_probabilities ['0.15', '0.20', '0.65']\n",
      "probabilities: ['0.08', '0.22', '0.47']\n",
      "normalized_probabilities ['0.10', '0.28', '0.61']\n",
      "probabilities: ['0.06', '0.07', '0.10']\n",
      "normalized_probabilities ['0.27', '0.30', '0.42']\n",
      "probabilities: ['0.12', '0.21', '0.39']\n",
      "normalized_probabilities ['0.17', '0.29', '0.54']\n",
      "probabilities: ['0.01', '0.01', '0.96']\n",
      "normalized_probabilities ['0.01', '0.01', '0.97']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.01', '0.04', '0.94']\n",
      "normalized_probabilities ['0.01', '0.04', '0.95']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.03', '0.40', '0.55']\n",
      "normalized_probabilities ['0.03', '0.40', '0.57']\n",
      "probabilities: ['0.07', '0.09', '0.15']\n",
      "normalized_probabilities ['0.23', '0.28', '0.49']\n",
      "probabilities: ['0.01', '0.03', '0.93']\n",
      "normalized_probabilities ['0.01', '0.04', '0.96']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities: ['0.17', '0.17', '0.19']\n",
      "normalized_probabilities ['0.32', '0.32', '0.36']\n",
      "probabilities: ['0.12', '0.23', '0.47']\n",
      "normalized_probabilities ['0.15', '0.28', '0.58']\n",
      "probabilities: ['0.18', '0.25', '0.29']\n",
      "normalized_probabilities ['0.25', '0.34', '0.40']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.01', '0.01', '0.01']\n",
      "normalized_probabilities ['0.28', '0.34', '0.39']\n",
      "probabilities: ['0.02', '0.03', '0.94']\n",
      "normalized_probabilities ['0.02', '0.03', '0.95']\n",
      "probabilities: ['0.07', '0.12', '0.71']\n",
      "normalized_probabilities ['0.08', '0.13', '0.79']\n",
      "probabilities: ['0.08', '0.12', '0.36']\n",
      "normalized_probabilities ['0.15', '0.21', '0.64']\n",
      "probabilities: ['0.08', '0.11', '0.13']\n",
      "normalized_probabilities ['0.24', '0.35', '0.41']\n",
      "probabilities: ['0.11', '0.14', '0.19']\n",
      "normalized_probabilities ['0.24', '0.32', '0.44']\n",
      "probabilities: ['0.10', '0.10', '0.34']\n",
      "normalized_probabilities ['0.19', '0.19', '0.62']\n",
      "probabilities: ['0.09', '0.13', '0.35']\n",
      "normalized_probabilities ['0.16', '0.22', '0.62']\n",
      "probabilities: ['0.13', '0.16', '0.32']\n",
      "normalized_probabilities ['0.21', '0.27', '0.52']\n",
      "probabilities: ['0.00', '0.01', '0.99']\n",
      "normalized_probabilities ['0.00', '0.01', '0.99']\n",
      "probabilities: ['0.08', '0.09', '0.15']\n",
      "normalized_probabilities ['0.26', '0.27', '0.47']\n",
      "probabilities: ['0.05', '0.27', '0.63']\n",
      "normalized_probabilities ['0.05', '0.29', '0.67']\n",
      "probabilities: ['0.06', '0.07', '0.08']\n",
      "normalized_probabilities ['0.28', '0.32', '0.39']\n",
      "probabilities: ['0.08', '0.09', '0.21']\n",
      "normalized_probabilities ['0.22', '0.24', '0.55']\n",
      "probabilities: ['0.09', '0.20', '0.41']\n",
      "normalized_probabilities ['0.12', '0.29', '0.59']\n",
      "probabilities: ['0.09', '0.12', '0.16']\n",
      "normalized_probabilities ['0.25', '0.31', '0.44']\n",
      "probabilities: ['0.00', '0.06', '0.94']\n",
      "normalized_probabilities ['0.00', '0.06', '0.94']\n",
      "probabilities: ['0.12', '0.12', '0.62']\n",
      "normalized_probabilities ['0.14', '0.14', '0.72']\n",
      "probabilities: ['0.05', '0.21', '0.58']\n",
      "normalized_probabilities ['0.06', '0.25', '0.69']\n",
      "probabilities: ['0.04', '0.19', '0.57']\n",
      "normalized_probabilities ['0.05', '0.24', '0.71']\n",
      "probabilities: ['0.05', '0.37', '0.57']\n",
      "normalized_probabilities ['0.05', '0.38', '0.57']\n",
      "probabilities: ['0.11', '0.11', '0.38']\n",
      "normalized_probabilities ['0.18', '0.19', '0.63']\n",
      "probabilities: ['0.12', '0.12', '0.22']\n",
      "normalized_probabilities ['0.26', '0.26', '0.47']\n",
      "probabilities: ['0.04', '0.04', '0.14']\n",
      "normalized_probabilities ['0.16', '0.18', '0.65']\n",
      "probabilities: ['0.12', '0.25', '0.34']\n",
      "normalized_probabilities ['0.17', '0.36', '0.48']\n",
      "probabilities: ['0.06', '0.08', '0.36']\n",
      "normalized_probabilities ['0.13', '0.15', '0.72']\n",
      "probabilities: ['0.04', '0.05', '0.13']\n",
      "normalized_probabilities ['0.20', '0.22', '0.58']\n",
      "probabilities: ['0.03', '0.03', '0.06']\n",
      "normalized_probabilities ['0.26', '0.26', '0.48']\n",
      "probabilities: ['0.03', '0.05', '0.77']\n",
      "normalized_probabilities ['0.03', '0.06', '0.91']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '0.00', '0.99']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.41', '0.59']\n",
      "normalized_probabilities ['0.00', '0.41', '0.59']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.08', '0.08', '0.12']\n",
      "normalized_probabilities ['0.27', '0.29', '0.44']\n",
      "probabilities: ['0.09', '0.12', '0.19']\n",
      "normalized_probabilities ['0.24', '0.29', '0.47']\n",
      "probabilities: ['0.05', '0.05', '0.06']\n",
      "normalized_probabilities ['0.31', '0.31', '0.38']\n",
      "probabilities: ['0.05', '0.11', '0.36']\n",
      "normalized_probabilities ['0.09', '0.21', '0.70']\n",
      "probabilities: ['0.10', '0.20', '0.38']\n",
      "normalized_probabilities ['0.15', '0.29', '0.56']\n",
      "probabilities: ['0.02', '0.07', '0.85']\n",
      "normalized_probabilities ['0.02', '0.08', '0.90']\n",
      "probabilities: ['0.03', '0.11', '0.79']\n",
      "normalized_probabilities ['0.04', '0.12', '0.85']\n",
      "probabilities: ['0.08', '0.09', '0.11']\n",
      "normalized_probabilities ['0.28', '0.33', '0.38']\n",
      "probabilities: ['0.04', '0.11', '0.49']\n",
      "normalized_probabilities ['0.06', '0.16', '0.77']\n",
      "probabilities: ['0.05', '0.11', '0.14']\n",
      "normalized_probabilities ['0.16', '0.38', '0.46']\n",
      "probabilities: ['0.10', '0.11', '0.19']\n",
      "normalized_probabilities ['0.24', '0.29', '0.47']\n",
      "probabilities: ['0.06', '0.09', '0.58']\n",
      "normalized_probabilities ['0.08', '0.12', '0.80']\n",
      "probabilities: ['0.05', '0.06', '0.10']\n",
      "normalized_probabilities ['0.25', '0.28', '0.47']\n",
      "probabilities: ['0.07', '0.07', '0.09']\n",
      "normalized_probabilities ['0.31', '0.32', '0.37']\n",
      "probabilities: ['0.08', '0.17', '0.44']\n",
      "normalized_probabilities ['0.11', '0.25', '0.64']\n",
      "probabilities: ['0.17', '0.18', '0.49']\n",
      "normalized_probabilities ['0.20', '0.21', '0.58']\n",
      "probabilities: ['0.03', '0.05', '0.78']\n",
      "normalized_probabilities ['0.03', '0.06', '0.91']\n",
      "probabilities: ['0.09', '0.19', '0.43']\n",
      "normalized_probabilities ['0.13', '0.27', '0.60']\n",
      "probabilities: ['0.13', '0.20', '0.27']\n",
      "normalized_probabilities ['0.22', '0.34', '0.44']\n",
      "probabilities: ['0.06', '0.08', '0.08']\n",
      "normalized_probabilities ['0.27', '0.35', '0.38']\n",
      "probabilities: ['0.03', '0.03', '0.37']\n",
      "normalized_probabilities ['0.07', '0.08', '0.85']\n",
      "probabilities: ['0.21', '0.28', '0.30']\n",
      "normalized_probabilities ['0.26', '0.36', '0.38']\n",
      "probabilities: ['0.00', '0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '0.00', '0.99']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.09', '0.12', '0.43']\n",
      "normalized_probabilities ['0.14', '0.19', '0.67']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.01', '0.99']\n",
      "normalized_probabilities ['0.00', '0.01', '0.99']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.04', '0.05', '0.05']\n",
      "normalized_probabilities ['0.30', '0.33', '0.37']\n",
      "probabilities: ['0.04', '0.06', '0.17']\n",
      "normalized_probabilities ['0.14', '0.22', '0.64']\n",
      "probabilities: ['0.07', '0.08', '0.27']\n",
      "normalized_probabilities ['0.16', '0.20', '0.64']\n",
      "probabilities: ['0.04', '0.05', '0.31']\n",
      "normalized_probabilities ['0.09', '0.12', '0.79']\n",
      "probabilities: ['0.16', '0.19', '0.20']\n",
      "normalized_probabilities ['0.29', '0.35', '0.36']\n",
      "probabilities: ['0.11', '0.23', '0.55']\n",
      "normalized_probabilities ['0.13', '0.26', '0.62']\n",
      "probabilities: ['0.12', '0.22', '0.28']\n",
      "normalized_probabilities ['0.20', '0.35', '0.45']\n",
      "probabilities: ['0.06', '0.08', '0.25']\n",
      "normalized_probabilities ['0.16', '0.20', '0.64']\n",
      "probabilities: ['0.10', '0.26', '0.39']\n",
      "normalized_probabilities ['0.13', '0.34', '0.53']\n",
      "probabilities: ['0.08', '0.12', '0.39']\n",
      "normalized_probabilities ['0.13', '0.20', '0.67']\n",
      "probabilities: ['0.13', '0.16', '0.66']\n",
      "normalized_probabilities ['0.14', '0.17', '0.70']\n",
      "probabilities: ['0.09', '0.12', '0.21']\n",
      "normalized_probabilities ['0.21', '0.28', '0.50']\n",
      "probabilities: ['0.07', '0.08', '0.20']\n",
      "normalized_probabilities ['0.20', '0.23', '0.57']\n",
      "probabilities: ['0.05', '0.06', '0.09']\n",
      "normalized_probabilities ['0.26', '0.31', '0.43']\n",
      "probabilities: ['0.11', '0.14', '0.37']\n",
      "normalized_probabilities ['0.17', '0.22', '0.61']\n",
      "probabilities: ['0.09', '0.11', '0.18']\n",
      "normalized_probabilities ['0.25', '0.28', '0.47']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.00', '0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '0.00', '1.00']\n",
      "probabilities: ['0.13', '0.20', '0.27']\n",
      "normalized_probabilities ['0.21', '0.34', '0.45']\n",
      "probabilities: ['0.07', '0.15', '0.57']\n",
      "normalized_probabilities ['0.08', '0.19', '0.73']\n",
      "probabilities: ['0.02', '0.02', '0.94']\n",
      "normalized_probabilities ['0.02', '0.02', '0.96']\n",
      "probabilities: ['0.03', '0.09', '0.86']\n",
      "normalized_probabilities ['0.03', '0.09', '0.88']\n",
      "probabilities: ['0.02', '0.03', '0.93']\n",
      "normalized_probabilities ['0.02', '0.03', '0.95']\n",
      "probabilities: ['0.07', '0.20', '0.29']\n",
      "normalized_probabilities ['0.12', '0.36', '0.52']\n",
      "probabilities: ['0.05', '0.14', '0.68']\n",
      "normalized_probabilities ['0.06', '0.16', '0.78']\n",
      "probabilities: ['0.04', '0.06', '0.73']\n",
      "normalized_probabilities ['0.05', '0.07', '0.88']\n",
      "probabilities: ['0.05', '0.12', '0.75']\n",
      "normalized_probabilities ['0.05', '0.13', '0.81']\n",
      "probabilities: ['0.07', '0.34', '0.39']\n",
      "normalized_probabilities ['0.09', '0.43', '0.49']\n",
      "probabilities: ['0.08', '0.11', '0.54']\n",
      "normalized_probabilities ['0.11', '0.15', '0.74']\n",
      "probabilities: ['0.03', '0.27', '0.64']\n",
      "normalized_probabilities ['0.03', '0.29', '0.68']\n",
      "probabilities: ['0.08', '0.08', '0.52']\n",
      "normalized_probabilities ['0.11', '0.12', '0.77']\n",
      "probabilities: ['0.01', '0.01', '0.97']\n",
      "normalized_probabilities ['0.01', '0.01', '0.98']\n",
      "Результат сохранён в атрибут generated_flow\n"
     ]
    }
   ],
   "source": [
    "mymodel_hiphop.generate_seq_with_randomness(200,3) #Генерация потока слов \n",
    "#В параметрах длина потока и число наиболее вероятных слов, из которого выбирается следующее слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'радио тренировочные мясо рекламу расширила как щелкаю бро раком густая падла они культуру просторы плавится с кто века знакомый откройте глаза страна модели испытаний забирает спасибо глаз и я не знаю мэн именно ты всегда слушаешь и че там занимайте в каменном ноги смерть еее знаний подписка где смешалось мы тонем пополам разделения и мигают пять отсчёт солнце просты на потери так закрою путь это про укурку это осталось сказал лживые денег и любит джаз секунду что меньше головой это удар скандалам послушай ак47 я с собой на ковчег остаться человеком среди сволочей в душ ярко тюбик трава и отрабатывать у ставил те кто внезапно не пускай даже не скрывают на встречу такие в одним небом процесс кемто всех аккуратней не осталось ни чего ты не может это не комильфо или моветон и ктото экономил на дом а я сделал пришло итак сон но я без тебя я не могу жить я начал примером брать хватит надо не выпустят отсюда он не приедет в харьков да кстати кто ты скажешь как соседи стоит ночь в главной мир сказать идет в что 2я останется резину в такую восковой тёмных кривляется окно до золотых роли пить по штаны суну на блеске отложи'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel_hiphop.generated_flow #Сгенерированный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Пробуем искать рифмы к слову  щелкаю\n",
      "Найдено рифм 8\n",
      "1067\n",
      "Пробуем искать рифмы к слову  плавится\n",
      "Найдено рифм 263\n",
      "1017\n",
      "Пробуем искать рифмы к слову  испытаний\n",
      "Найдено рифм 44\n",
      "942\n",
      "Пробуем искать рифмы к слову  именно\n",
      "Найдено рифм 28\n",
      "895\n",
      "Пробуем искать рифмы к слову  еее\n",
      "Найдено рифм 0\n",
      "Пробуем искать рифмы к слову  еее\n",
      "Найдено рифм 0\n",
      "Пробуем искать рифмы к слову  еее\n",
      "Найдено рифм 0\n",
      "Пробуем искать рифмы к слову  еее\n",
      "Найдено рифм 0\n",
      "Пробуем искать рифмы к слову  еее\n",
      "Найдено рифм 0\n",
      "Пробуем искать рифмы к слову  еее\n",
      "Найдено рифм 0\n",
      "Пробуем искать рифмы к слову  смерть\n",
      "Найдено рифм 1\n",
      "821\n",
      "Пробуем искать рифмы к слову  отсчёт\n",
      "Найдено рифм 3\n",
      "747\n",
      "Пробуем искать рифмы к слову  осталось\n",
      "Найдено рифм 38\n",
      "679\n",
      "Пробуем искать рифмы к слову  удар\n",
      "Найдено рифм 2\n",
      "628\n",
      "Пробуем искать рифмы к слову  ковчег\n",
      "Найдено рифм 1\n",
      "569\n",
      "Пробуем искать рифмы к слову  трава\n",
      "Найдено рифм 8\n",
      "522\n",
      "Пробуем искать рифмы к слову  пускай\n",
      "Найдено рифм 7\n",
      "479\n",
      "Пробуем искать рифмы к слову  небом\n",
      "Найдено рифм 6\n",
      "420\n",
      "Пробуем искать рифмы к слову  чего\n",
      "Найдено рифм 33\n",
      "383\n",
      "Пробуем искать рифмы к слову  моветон\n",
      "Найдено рифм 9\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "mymodel_hiphop.create_verse(6) #Добавление рифм и разбиение на строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "радио тренировочные мясо рекламу расширила как щелкаю\n",
      "бро втыкаю\n",
      "густая падла они культуру просторы плавится\n",
      "с палится\n",
      "века знакомый откройте глаза страна модели испытаний\n",
      "забирает спасибо гуляний\n",
      "и я не знаю мэн именно\n",
      "ты всегда слушаешь и мгновенно\n",
      "там занимайте в каменном ноги смерть\n",
      "еее знаний подписка где смешалось круговерть\n",
      "тонем пополам разделения и мигают пять отсчёт\n",
      "солнце просты на потери учёт\n",
      "закрою путь это про укурку это осталось\n",
      "сказал лживые денег и разлетелось\n",
      "джаз секунду что меньше головой это удар\n",
      "краснодар\n",
      "послушай ак47 я с собой на ковчег\n",
      "остаться человеком витчег\n",
      "сволочей в душ ярко тюбик трава\n",
      "и удава\n",
      "у ставил те кто внезапно не пускай\n",
      "даже напичкай\n",
      "скрывают на встречу такие в одним небом\n",
      "процесс кемто ознобом\n",
      "аккуратней не осталось ни чего\n",
      "ты соседнего\n",
      "может это не комильфо или моветон\n",
      "брайтон\n"
     ]
    }
   ],
   "source": [
    "mymodel_hiphop.print_verses() #Вывод стиха"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Обучение на детских стихах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel_hiphop.train_model(128,ep_num=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel_hiphop.model.save('loss15extendedlyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel_hiphop.save_weights('loss15extendedlyr-w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel_child_poetry = LyricsGenModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel_child_poetry.set_length(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных\n",
      "Найдено песен: 83009\n",
      "Получено токенов из корпуса текста: 83009\n",
      "['раздватричетырепять будем пальчики', 'будем пальчики считать', 'пальчики считать крепкие', 'считать крепкие дружные', 'крепкие дружные все', 'дружные все такие', 'все такие нужные', 'такие нужные на', 'нужные на другой', 'на другой руке']\n",
      "Размер словаря: 18512\n"
     ]
    }
   ],
   "source": [
    "mymodel_child_poetry.load_data('stihi-ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 2, 50)             925600    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 2, 100)            45300     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 100)               60300     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 18512)             1869712   \n",
      "=================================================================\n",
      "Total params: 2,911,012\n",
      "Trainable params: 2,911,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mymodel_child_poetry.load_model('ch-model-seq3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel_child_poetry.load_weights('ch-weights-seq3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel_child_poetry.get_seed_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities: ['0.08', '0.27']\n",
      "normalized_probabilities ['0.23', '0.77']\n",
      "probabilities: ['0.16', '0.61']\n",
      "normalized_probabilities ['0.21', '0.79']\n",
      "probabilities: ['0.20', '0.77']\n",
      "normalized_probabilities ['0.21', '0.79']\n",
      "probabilities: ['0.13', '0.33']\n",
      "normalized_probabilities ['0.28', '0.72']\n",
      "probabilities: ['0.12', '0.38']\n",
      "normalized_probabilities ['0.24', '0.76']\n",
      "probabilities: ['0.06', '0.65']\n",
      "normalized_probabilities ['0.09', '0.91']\n",
      "probabilities: ['0.23', '0.25']\n",
      "normalized_probabilities ['0.48', '0.52']\n",
      "probabilities: ['0.16', '0.17']\n",
      "normalized_probabilities ['0.49', '0.51']\n",
      "probabilities: ['0.12', '0.12']\n",
      "normalized_probabilities ['0.50', '0.50']\n",
      "probabilities: ['0.06', '0.09']\n",
      "normalized_probabilities ['0.38', '0.62']\n",
      "probabilities: ['0.03', '0.87']\n",
      "normalized_probabilities ['0.04', '0.96']\n",
      "probabilities: ['0.01', '0.97']\n",
      "normalized_probabilities ['0.01', '0.99']\n",
      "probabilities: ['0.03', '0.90']\n",
      "normalized_probabilities ['0.04', '0.96']\n",
      "probabilities: ['0.07', '0.07']\n",
      "normalized_probabilities ['0.49', '0.51']\n",
      "probabilities: ['0.16', '0.56']\n",
      "normalized_probabilities ['0.22', '0.78']\n",
      "probabilities: ['0.05', '0.06']\n",
      "normalized_probabilities ['0.45', '0.55']\n",
      "probabilities: ['0.19', '0.21']\n",
      "normalized_probabilities ['0.47', '0.53']\n",
      "probabilities: ['0.18', '0.37']\n",
      "normalized_probabilities ['0.33', '0.67']\n",
      "probabilities: ['0.28', '0.47']\n",
      "normalized_probabilities ['0.38', '0.62']\n",
      "probabilities: ['0.09', '0.22']\n",
      "normalized_probabilities ['0.29', '0.71']\n",
      "probabilities: ['0.23', '0.46']\n",
      "normalized_probabilities ['0.33', '0.67']\n",
      "probabilities: ['0.16', '0.42']\n",
      "normalized_probabilities ['0.28', '0.72']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.14', '0.80']\n",
      "normalized_probabilities ['0.15', '0.85']\n",
      "probabilities: ['0.22', '0.23']\n",
      "normalized_probabilities ['0.48', '0.52']\n",
      "probabilities: ['0.02', '0.90']\n",
      "normalized_probabilities ['0.02', '0.98']\n",
      "probabilities: ['0.01', '0.95']\n",
      "normalized_probabilities ['0.01', '0.99']\n",
      "probabilities: ['0.01', '0.98']\n",
      "normalized_probabilities ['0.01', '0.99']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.01', '0.93']\n",
      "normalized_probabilities ['0.01', '0.99']\n",
      "probabilities: ['0.07', '0.28']\n",
      "normalized_probabilities ['0.20', '0.80']\n",
      "probabilities: ['0.12', '0.45']\n",
      "normalized_probabilities ['0.22', '0.78']\n",
      "probabilities: ['0.40', '0.55']\n",
      "normalized_probabilities ['0.43', '0.57']\n",
      "probabilities: ['0.06', '0.71']\n",
      "normalized_probabilities ['0.07', '0.93']\n",
      "probabilities: ['0.01', '0.97']\n",
      "normalized_probabilities ['0.01', '0.99']\n",
      "probabilities: ['0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.08', '0.83']\n",
      "normalized_probabilities ['0.09', '0.91']\n",
      "probabilities: ['0.18', '0.38']\n",
      "normalized_probabilities ['0.33', '0.67']\n",
      "probabilities: ['0.02', '0.83']\n",
      "normalized_probabilities ['0.03', '0.97']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.08', '0.10']\n",
      "normalized_probabilities ['0.44', '0.56']\n",
      "probabilities: ['0.06', '0.07']\n",
      "normalized_probabilities ['0.47', '0.53']\n",
      "probabilities: ['0.07', '0.10']\n",
      "normalized_probabilities ['0.39', '0.61']\n",
      "probabilities: ['0.10', '0.12']\n",
      "normalized_probabilities ['0.46', '0.54']\n",
      "probabilities: ['0.22', '0.26']\n",
      "normalized_probabilities ['0.46', '0.54']\n",
      "probabilities: ['0.13', '0.46']\n",
      "normalized_probabilities ['0.22', '0.78']\n",
      "probabilities: ['0.02', '0.97']\n",
      "normalized_probabilities ['0.02', '0.98']\n",
      "probabilities: ['0.09', '0.60']\n",
      "normalized_probabilities ['0.13', '0.87']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.11', '0.82']\n",
      "normalized_probabilities ['0.12', '0.88']\n",
      "probabilities: ['0.28', '0.38']\n",
      "normalized_probabilities ['0.42', '0.58']\n",
      "probabilities: ['0.13', '0.81']\n",
      "normalized_probabilities ['0.14', '0.86']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.08', '0.30']\n",
      "normalized_probabilities ['0.21', '0.79']\n",
      "probabilities: ['0.09', '0.62']\n",
      "normalized_probabilities ['0.13', '0.87']\n",
      "probabilities: ['0.20', '0.55']\n",
      "normalized_probabilities ['0.27', '0.73']\n",
      "probabilities: ['0.11', '0.28']\n",
      "normalized_probabilities ['0.28', '0.72']\n",
      "probabilities: ['0.08', '0.08']\n",
      "normalized_probabilities ['0.49', '0.51']\n",
      "probabilities: ['0.08', '0.54']\n",
      "normalized_probabilities ['0.13', '0.87']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.20', '0.42']\n",
      "normalized_probabilities ['0.32', '0.68']\n",
      "probabilities: ['0.30', '0.38']\n",
      "normalized_probabilities ['0.44', '0.56']\n",
      "probabilities: ['0.02', '0.95']\n",
      "normalized_probabilities ['0.02', '0.98']\n",
      "probabilities: ['0.20', '0.51']\n",
      "normalized_probabilities ['0.28', '0.72']\n",
      "probabilities: ['0.22', '0.60']\n",
      "normalized_probabilities ['0.27', '0.73']\n",
      "probabilities: ['0.22', '0.60']\n",
      "normalized_probabilities ['0.27', '0.73']\n",
      "probabilities: ['0.06', '0.16']\n",
      "normalized_probabilities ['0.25', '0.75']\n",
      "probabilities: ['0.17', '0.69']\n",
      "normalized_probabilities ['0.20', '0.80']\n",
      "probabilities: ['0.17', '0.75']\n",
      "normalized_probabilities ['0.19', '0.81']\n",
      "probabilities: ['0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.01', '0.97']\n",
      "normalized_probabilities ['0.01', '0.99']\n",
      "probabilities: ['0.01', '0.99']\n",
      "normalized_probabilities ['0.01', '0.99']\n",
      "probabilities: ['0.26', '0.52']\n",
      "normalized_probabilities ['0.34', '0.66']\n",
      "probabilities: ['0.04', '0.09']\n",
      "normalized_probabilities ['0.32', '0.68']\n",
      "probabilities: ['0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.11', '0.17']\n",
      "normalized_probabilities ['0.39', '0.61']\n",
      "probabilities: ['0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.00', '0.99']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.16', '0.18']\n",
      "normalized_probabilities ['0.46', '0.54']\n",
      "probabilities: ['0.07', '0.81']\n",
      "normalized_probabilities ['0.07', '0.93']\n",
      "probabilities: ['0.00', '1.00']\n",
      "normalized_probabilities ['0.00', '1.00']\n",
      "probabilities: ['0.17', '0.18']\n",
      "normalized_probabilities ['0.48', '0.52']\n",
      "probabilities: ['0.10', '0.79']\n",
      "normalized_probabilities ['0.11', '0.89']\n",
      "probabilities: ['0.09', '0.17']\n",
      "normalized_probabilities ['0.35', '0.65']\n",
      "probabilities: ['0.11', '0.43']\n",
      "normalized_probabilities ['0.21', '0.79']\n",
      "probabilities: ['0.16', '0.19']\n",
      "normalized_probabilities ['0.46', '0.54']\n",
      "probabilities: ['0.19', '0.27']\n",
      "normalized_probabilities ['0.42', '0.58']\n",
      "probabilities: ['0.03', '0.04']\n",
      "normalized_probabilities ['0.45', '0.55']\n",
      "probabilities: ['0.06', '0.09']\n",
      "normalized_probabilities ['0.41', '0.59']\n",
      "probabilities: ['0.19', '0.51']\n",
      "normalized_probabilities ['0.27', '0.73']\n",
      "probabilities: ['0.07', '0.11']\n",
      "normalized_probabilities ['0.40', '0.60']\n",
      "probabilities: ['0.09', '0.17']\n",
      "normalized_probabilities ['0.34', '0.66']\n",
      "probabilities: ['0.21', '0.34']\n",
      "normalized_probabilities ['0.38', '0.62']\n",
      "probabilities: ['0.19', '0.43']\n",
      "normalized_probabilities ['0.31', '0.69']\n",
      "probabilities: ['0.19', '0.28']\n",
      "normalized_probabilities ['0.40', '0.60']\n",
      "probabilities: ['0.10', '0.14']\n",
      "normalized_probabilities ['0.42', '0.58']\n",
      "Результат сохранён в атрибут generated_flow\n"
     ]
    }
   ],
   "source": [
    "mymodel_child_poetry.generate_seq_with_randomness(100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'другая тридцать лет работать подшефный петю здесь как будто месяц май а паренёк раздался класс со взрослой лягушки же пыли напишу ликуют в третьем ура мы победили кстати ли некстати ли скажу начистоту такие показатели грязнят всю чистоту не поверила а мальчишки в путь не по дням по свету возвратятся в книжку вести от читателей от вас принимается танцуют и и одно только слово твердит айболит лимпопо лимпопо лимпопо и акула каракула распахнула злую пасть вы в конце концов взмолился андрей петрович опоздаем торопят артисты борисов достаёт не пьёт он в ответ в кровать предлагает день начинаются света лида это'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel_child_poetry.generated_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "583\n",
      "583\n",
      "532\n",
      "465\n",
      "430\n",
      "374\n",
      "332\n",
      "280\n",
      "210\n",
      "166\n",
      "86\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "mymodel_child_poetry.create_verse(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "другая тридцать\n",
      "лет работать\n",
      "подшефный петю здесь\n",
      "проснитесь\n",
      "подшефный петю здесь\n",
      "как будто месяц май а смейтесь\n",
      "раздался класс со взрослой лягушки же пыли\n",
      "напишу ликуют в жилибыли\n",
      "ура мы победили кстати\n",
      "ли некстати\n",
      "ли скажу начистоту\n",
      "такие показатели грязнят всю чистоту\n",
      "не поверила а мальчишки в путь\n",
      "не по тронуть\n",
      "по свету возвратятся в книжку вести\n",
      "пусти\n",
      "вас принимается танцуют и и одно только\n",
      "слово твердит айболит тихонько\n",
      "лимпопо лимпопо и акула каракула\n",
      "распахнула\n",
      "злую пасть вы в конце концов взмолился\n",
      "андрей петрович опоздаем торопят поправился\n",
      "борисов достаёт не пьёт он в ответ\n",
      "цвет\n"
     ]
    }
   ],
   "source": [
    "mymodel_child_poetry.print_verses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель создана:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 2, 50)             925600    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 2, 100)            45300     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 100)               60300     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 18512)             1869712   \n",
      "=================================================================\n",
      "Total params: 2,911,012\n",
      "Trainable params: 2,911,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mymodel_child_poetry.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "83006/83006 [==============================] - 182s 2ms/step - loss: 2.6887 - acc: 0.4648\n",
      "Epoch 2/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.5765 - acc: 0.4833\n",
      "Epoch 3/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.4881 - acc: 0.4973\n",
      "Epoch 4/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.4033 - acc: 0.5104\n",
      "Epoch 5/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.3323 - acc: 0.5197\n",
      "Epoch 6/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.2637 - acc: 0.5315\n",
      "Epoch 7/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.2002 - acc: 0.5443\n",
      "Epoch 8/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.1404 - acc: 0.5501\n",
      "Epoch 9/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.0862 - acc: 0.5604\n",
      "Epoch 10/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 2.0342 - acc: 0.5694\n",
      "Epoch 11/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 1.9912 - acc: 0.5746\n",
      "Epoch 12/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 1.9412 - acc: 0.5828\n",
      "Epoch 13/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 1.8984 - acc: 0.5889\n",
      "Epoch 14/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 1.8568 - acc: 0.5969\n",
      "Epoch 15/20\n",
      "83006/83006 [==============================] - 182s 2ms/step - loss: 1.8175 - acc: 0.6031\n",
      "Epoch 16/20\n",
      "83006/83006 [==============================] - 183s 2ms/step - loss: 1.7847 - acc: 0.6085\n",
      "Epoch 17/20\n",
      "83006/83006 [==============================] - 182s 2ms/step - loss: 1.7490 - acc: 0.6133\n",
      "Epoch 18/20\n",
      "83006/83006 [==============================] - 189s 2ms/step - loss: 1.7164 - acc: 0.6184\n",
      "Epoch 19/20\n",
      "83006/83006 [==============================] - 188s 2ms/step - loss: 1.6850 - acc: 0.6241\n",
      "Epoch 20/20\n",
      "83006/83006 [==============================] - 187s 2ms/step - loss: 1.6543 - acc: 0.6296\n"
     ]
    }
   ],
   "source": [
    "mymodel_ch.train_model(b_size=128,ep_num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel_ch.save_model('ch-model-seq3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel_ch.save_weights('ch-weights-seq3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
